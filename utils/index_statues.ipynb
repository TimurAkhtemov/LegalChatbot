{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain nltk chromadb langchain-experimental langchain-community \\\n",
    "    sentence-transformers google-generativeai python-dotenv rich chromadb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set path relative to notebook location\n",
    "file_path = Path(\"../data/processed/processed_nj_statutes.json\")\n",
    "\n",
    "# Optional: confirm it exists\n",
    "if not file_path.exists():\n",
    "    raise FileNotFoundError(f\"Couldn't find file at: {file_path.resolve()}\")\n",
    "\n",
    "# Load the JSON\n",
    "with open(file_path, \"r\") as f:\n",
    "    statutes = json.load(f)\n",
    "\n",
    "print(\"‚úÖ File loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Sequence\n",
    "\n",
    "# Set your API key\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "class GeminiEmbeddingFunction:\n",
    "    def __call__(self, input: Sequence[str]) -> List[List[float]]:\n",
    "        # Defaulting to retrieval_document task type for indexing\n",
    "        return [self.embed_text(text, task_type=\"retrieval_document\") for text in input]\n",
    "\n",
    "    def embed_text(self, text: str, task_type=\"retrieval_document\") -> list[float]:\n",
    "        response = genai.embed_content(\n",
    "            model=\"models/embedding-001\",\n",
    "            content=text,\n",
    "            task_type=task_type\n",
    "        )\n",
    "        return response[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_statute_text(text: str) -> str:\n",
    "    # Remove isolated digits/headers like \"12.\" at the start of a line\n",
    "    text = re.sub(r'^\\s*\\d+\\.\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalize extra newlines and tabs\n",
    "    text = re.sub(r'\\t', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,        # adjust as needed (~750-1000 chars = 250-350 tokens)\n",
    "    chunk_overlap=100       # overlap to preserve context\n",
    ")\n",
    "\n",
    "documents = []\n",
    "\n",
    "for title in statutes:\n",
    "    for section in title[\"sections\"]:\n",
    "        cleaned_text = clean_statute_text(section[\"text\"])  # optional cleaner from earlier\n",
    "        heading = section[\"heading\"]\n",
    "        section_id = section[\"section\"]\n",
    "\n",
    "        chunks = text_splitter.create_documents([cleaned_text])\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=chunk.page_content.strip(),\n",
    "                    metadata={\n",
    "                        \"title\": title[\"title\"],\n",
    "                        \"section\": section_id,\n",
    "                        \"heading\": heading,\n",
    "                        \"chunk_id\": idx\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    meta = doc.metadata\n",
    "    print(f\"--- Chunk #{i+1} ---\")\n",
    "    print(f\"Title:   {meta.get('title')}\")\n",
    "    print(f\"Section: {meta.get('section')}\")\n",
    "    print(f\"Heading: {meta.get('heading')}\")\n",
    "    print(f\"Chunk ID: {meta.get('chunk_id')}\\n\")\n",
    "    print(doc.page_content.strip())\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize Chroma\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"nj_statutes_test_chunks\",\n",
    "    embedding_function=GeminiEmbeddingFunction()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration\n",
    "# -------------------------------\n",
    "PROGRESS_CHECK = 10         # Print progress every N documents\n",
    "BATCH_SIZE = 15             # Documents per indexing batch\n",
    "RETRY_LIMIT = 3             # Max retries for failed batch\n",
    "RETRY_DELAY = 5             # Delay between retries in seconds\n",
    "\n",
    "# -------------------------------\n",
    "# Data Preparation\n",
    "# -------------------------------\n",
    "document_ids = []\n",
    "document_texts = []\n",
    "document_metadatas = []\n",
    "\n",
    "print(\"üîÑ Preparing documents for indexing...\")\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    doc_id = f\"statute_{doc.metadata['section']}_{doc.metadata['chunk_id']}\"\n",
    "    document_ids.append(doc_id)\n",
    "    document_texts.append(doc.page_content)\n",
    "    document_metadatas.append(doc.metadata)\n",
    "\n",
    "    if (i + 1) % PROGRESS_CHECK == 0:\n",
    "        percent = ((i + 1) / len(documents)) * 100\n",
    "        print(f\"üìÑ Processed {i + 1}/{len(documents)} documents ({percent:.1f}%)\")\n",
    "\n",
    "# -------------------------------\n",
    "# Batch Indexing with Retry Logic\n",
    "# -------------------------------\n",
    "print(\"\\nüöÄ Starting ChromaDB indexing...\")\n",
    "\n",
    "for i in range(0, len(document_ids), BATCH_SIZE):\n",
    "    end_idx = min(i + BATCH_SIZE, len(document_ids))\n",
    "    batch_ids = document_ids[i:end_idx]\n",
    "    batch_texts = document_texts[i:end_idx]\n",
    "    batch_metadatas = document_metadatas[i:end_idx]\n",
    "\n",
    "    batch_num = (i // BATCH_SIZE) + 1\n",
    "    success = False\n",
    "\n",
    "    for attempt in range(1, RETRY_LIMIT + 1):\n",
    "        try:\n",
    "            collection.add(\n",
    "                ids=batch_ids,\n",
    "                documents=batch_texts,\n",
    "                metadatas=batch_metadatas\n",
    "            )\n",
    "            print(f\"‚úÖ Indexed batch {batch_num}, documents {i + 1} to {end_idx}\")\n",
    "            success = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error on batch {batch_num} (attempt {attempt}): {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "\n",
    "    if not success:\n",
    "        print(f\"‚ùå Failed to index batch {batch_num} after {RETRY_LIMIT} attempts.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Completion\n",
    "# -------------------------------\n",
    "print(f\"\\nüéâ Successfully processed {len(document_ids)} documents in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user's question\n",
    "def embed_query(text):\n",
    "    response = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=text,\n",
    "        task_type=\"retrieval_query\"\n",
    "    )\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "# Retrieve from Chroma\n",
    "def retrieve_context(question, k=5):\n",
    "    query_embedding = embed_query(question)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Format the context from Chroma results\n",
    "def format_context(results):\n",
    "    chunks = results[\"documents\"][0]\n",
    "    return \"\\n\\n---\\n\\n\".join(chunks)\n",
    "\n",
    "# Ask Gemini to answer using context\n",
    "def ask_gemini(question, context):\n",
    "    prompt = f\"\"\"\n",
    "You are a legal assistant that answers based on official New Jersey law.\n",
    "\n",
    "Answer based on context provided but if the context does not contain information about the question, use your best judgement to answer the question\n",
    "\n",
    "Be detailed, this is a legal chatbot but for the general public. Keep the chat natural and conversational.  \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Create Gemini model instance\n",
    "        model = genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "\n",
    "        # Generate content with safety\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check for missing response\n",
    "        if not response.candidates:\n",
    "            return \"[ERROR: No response candidates returned.]\"\n",
    "\n",
    "        return response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR: Gemini call failed]\\nDetails: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who conducts the investigation in a private adoption case in NJ?\"\n",
    "results = retrieve_context(question)\n",
    "context = format_context(results)\n",
    "answer = ask_gemini(question, context)\n",
    "\n",
    "print(\"üîç Context Used:\\n\", context)\n",
    "print(\"\\nüß† Gemini's Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "questions = [\n",
    "    \"What happens if a child is not adopted through an approved agency in New Jersey?\",\n",
    "    \"Who conducts the investigation in a private adoption case in NJ?\",\n",
    "    \"Can a stepparent skip the agency investigation in an adoption case?\",\n",
    "    \"What does the preliminary hearing determine in a New Jersey adoption case?\",\n",
    "    \"When can the court immediately issue a judgment of adoption?\",\n",
    "    \"What responsibilities does the agency have after the preliminary hearing?\"\n",
    "]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"../logs/gemini_adoption_test_{timestamp}.log\"\n",
    "\n",
    "file_path = Path(log_file)\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, q in enumerate(questions, start=1):\n",
    "        results = retrieve_context(q)\n",
    "        context = format_context(results)\n",
    "        response = ask_gemini(q, context)\n",
    "\n",
    "        f.write(f\"--- Test #{i} ---\\n\")\n",
    "        f.write(f\"Q: {q}\\n\\n\")\n",
    "        f.write(f\"Context Used:\\n{context}\\n\\n\")\n",
    "        f.write(f\"Response:\\n{response}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "print(f\"‚úÖ All responses saved to'{log_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
